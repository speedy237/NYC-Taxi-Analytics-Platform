{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fe04e2-6c07-4732-9d4d-cf51464da595",
   "metadata": {},
   "source": [
    "# Silver Layer - Nettoyage & Enrichissement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e53a7eb-8f27-43dc-b0d0-5e6da1d9bd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Liste des tables dans le namespace bronze:\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o37.sql.\n: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: iceberg.bronze\n\tat org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:173)\n\tat org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:166)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.listTables(RESTSessionCatalog.java:388)\n\tat org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.listTables(BaseSessionCatalog.java:79)\n\tat org.apache.iceberg.rest.RESTCatalog.listTables(RESTCatalog.java:92)\n\tat org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:136)\n\tat org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:416)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat jdk.internal.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# V√©rification des tables cr√©√©es\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìã Liste des tables dans le namespace bronze:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHOW TABLES IN iceberg.bronze\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Pour une table sp√©cifique\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîç Description de la table taxi_trips_2024:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o37.sql.\n: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: iceberg.bronze\n\tat org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:173)\n\tat org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:166)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.listTables(RESTSessionCatalog.java:388)\n\tat org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.listTables(BaseSessionCatalog.java:79)\n\tat org.apache.iceberg.rest.RESTCatalog.listTables(RESTCatalog.java:92)\n\tat org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:136)\n\tat org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:416)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat jdk.internal.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# V√©rification des tables cr√©√©es\n",
    "print(\"üìã Liste des tables dans le namespace bronze:\")\n",
    "spark.sql(\"SHOW TABLES IN iceberg.bronze\").show()\n",
    "\n",
    "# Pour une table sp√©cifique\n",
    "print(\"\\nüîç Description de la table taxi_trips_2024:\")\n",
    "spark.sql(\"DESCRIBE EXTENDED iceberg.bronze.taxi_trips_2024\").show(truncate=False)\n",
    "\n",
    "# V√©rifier les donn√©es\n",
    "print(\"\\nüìä Aper√ßu des donn√©es (5 premi√®res lignes):\")\n",
    "spark.sql(\"SELECT * FROM iceberg.bronze.taxi_trips_2024 LIMIT 5\").show()\n",
    "\n",
    "# V√©rifier les partitions\n",
    "print(\"\\nüóÇÔ∏è Partitions de la table taxi_trips_2024:\")\n",
    "spark.sql(\"SHOW PARTITIONS iceberg.bronze.taxi_trips_2024\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d71fdbc4-76bc-40a3-970a-8b6c613c2fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©marrage Silver Layer (version unifi√©e)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Table taxi_trips_2024 non trouv√©e dans local.bronze ni iceberg.bronze",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 292\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trips_complete\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Ex√©cuter\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m \u001b[43mcreate_silver_tables_unified\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m, in \u001b[0;36mcreate_silver_tables_unified\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ D√©marrage Silver Layer (version unifi√©e)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# D√©tecter o√π sont les tables Bronze\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m trips_table \u001b[38;5;241m=\u001b[39m \u001b[43mget_bronze_table_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaxi_trips_2024\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m zones_table \u001b[38;5;241m=\u001b[39m get_bronze_table_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaxi_zones\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m weather_table \u001b[38;5;241m=\u001b[39m get_bronze_table_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather_2024\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mget_bronze_table_name\u001b[0;34m(base_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m non trouv√©e dans local.bronze ni iceberg.bronze\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Table taxi_trips_2024 non trouv√©e dans local.bronze ni iceberg.bronze"
     ]
    }
   ],
   "source": [
    "# notebooks/03_silver_processing_unified_fixed.py\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def get_bronze_table_name(base_name):\n",
    "    \"\"\"\n",
    "    Trouve automatiquement o√π se trouve la table Bronze\n",
    "    \"\"\"\n",
    "    possible_locations = [\n",
    "        f\"local.bronze.{base_name}\",\n",
    "        f\"iceberg.bronze.{base_name}\"\n",
    "    ]\n",
    "    \n",
    "    for table_name in possible_locations:\n",
    "        try:\n",
    "            # Tester si la table existe\n",
    "            spark.sql(f\"SELECT 1 FROM {table_name} LIMIT 1\")\n",
    "            print(f\"   üìç Table trouv√©e: {table_name}\")\n",
    "            return table_name\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    raise Exception(f\"Table {base_name} non trouv√©e dans local.bronze ni iceberg.bronze\")\n",
    "\n",
    "def create_silver_tables_unified():\n",
    "    \"\"\"\n",
    "    Version unifi√©e qui fonctionne avec les deux catalogues - CORRIG√âE\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ D√©marrage Silver Layer (version unifi√©e)\")\n",
    "    \n",
    "    # D√©tecter o√π sont les tables Bronze\n",
    "    trips_table = get_bronze_table_name(\"taxi_trips_2024\")\n",
    "    zones_table = get_bronze_table_name(\"taxi_zones\")\n",
    "    weather_table = get_bronze_table_name(\"weather_2024\")\n",
    "    \n",
    "    # D√©terminer o√π √©crire (utiliser le m√™me catalogue que la source)\n",
    "    catalog = trips_table.split('.')[0]  # \"local\" ou \"iceberg\"\n",
    "    print(f\"   üìÅ Catalogue cible: {catalog}\")\n",
    "    \n",
    "    # Cr√©er le namespace Silver si n√©cessaire\n",
    "    try:\n",
    "        spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {catalog}.silver\")\n",
    "        print(f\"‚úÖ Namespace {catalog}.silver cr√©√©/valid√©\")\n",
    "    except:\n",
    "        print(f\"‚ÑπÔ∏è  Namespace {catalog}.silver d√©j√† existant\")\n",
    "    \n",
    "    # 1. Nettoyage des trips\n",
    "    print(\"\\nüìä √âtape 1: Nettoyage des trips...\")\n",
    "    trips_df = spark.table(trips_table)\n",
    "    \n",
    "    print(f\"   Lignes brutes: {trips_df.count():,}\")\n",
    "    \n",
    "    # Nettoyage et enrichissement\n",
    "    trips_cleaned = trips_df.filter(\n",
    "        (F.col(\"trip_distance\") > 0) &\n",
    "        (F.col(\"trip_distance\") < 100) &\n",
    "        (F.col(\"total_amount\") > 0) &\n",
    "        (F.col(\"total_amount\") < 500) &\n",
    "        (F.col(\"passenger_count\") > 0) &\n",
    "        (F.col(\"passenger_count\") <= 6) &\n",
    "        (F.col(\"tpep_dropoff_datetime\") > F.col(\"tpep_pickup_datetime\"))\n",
    "    ).withColumn(\n",
    "        \"trip_duration_minutes\",\n",
    "        (F.unix_timestamp(\"tpep_dropoff_datetime\") - \n",
    "         F.unix_timestamp(\"tpep_pickup_datetime\")) / 60\n",
    "    ).withColumn(\n",
    "        \"speed_mph\",\n",
    "        F.when(\n",
    "            (F.col(\"trip_duration_minutes\") > 0) & (F.col(\"trip_distance\") > 0),\n",
    "            F.col(\"trip_distance\") / (F.col(\"trip_duration_minutes\") / 60)\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        \"is_weekend\",\n",
    "        F.when(F.dayofweek(\"tpep_pickup_datetime\").isin([1, 7]), 1).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"hour_of_day\", F.hour(\"tpep_pickup_datetime\")\n",
    "    ).withColumn(\n",
    "        \"year\", F.year(\"tpep_pickup_datetime\")\n",
    "    ).withColumn(\n",
    "        \"month\", F.month(\"tpep_pickup_datetime\")\n",
    "    ).withColumn(\n",
    "        \"pickup_date\", F.to_date(\"tpep_pickup_datetime\")\n",
    "    ).withColumn(\n",
    "        \"pickup_time\", F.date_format(\"tpep_pickup_datetime\", \"HH:mm:ss\")\n",
    "    )\n",
    "    \n",
    "    # Filtrer les valeurs aberrantes suppl√©mentaires\n",
    "    trips_cleaned = trips_cleaned.filter(\n",
    "        (F.col(\"trip_duration_minutes\") > 0) &\n",
    "        (F.col(\"trip_duration_minutes\") < 180) &\n",
    "        (F.col(\"speed_mph\") <= 80) &\n",
    "        (F.col(\"speed_mph\") >= 0)\n",
    "    )\n",
    "    \n",
    "    print(f\"   Lignes apr√®s nettoyage: {trips_cleaned.count():,}\")\n",
    "    \n",
    "    # √âcrire dans Silver\n",
    "    trips_cleaned.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"year\", \"month\") \\\n",
    "        .saveAsTable(f\"{catalog}.silver.trips_enriched\")\n",
    "    \n",
    "    print(f\"‚úÖ {catalog}.silver.trips_enriched cr√©√©e: {trips_cleaned.count():,} lignes\")\n",
    "    \n",
    "    # 2. Enrichissement avec zones\n",
    "    print(\"\\nüó∫Ô∏è  √âtape 2: Enrichissement avec zones...\")\n",
    "    zones_df = spark.table(zones_table)\n",
    "    \n",
    "    # Jointure avec zones de pickup\n",
    "    trips_with_zones = trips_cleaned.alias(\"t\").join(\n",
    "        zones_df.alias(\"pz\"),\n",
    "        F.col(\"t.PULocationID\") == F.col(\"pz.LocationID\"),\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        \"t.*\",\n",
    "        F.col(\"pz.zone\").alias(\"pickup_zone\"),\n",
    "        F.col(\"pz.borough\").alias(\"pickup_borough\")\n",
    "    )\n",
    "    \n",
    "    # Jointure avec zones de dropoff\n",
    "    trips_with_zones = trips_with_zones.alias(\"t\").join(\n",
    "        zones_df.alias(\"dz\"),\n",
    "        F.col(\"t.DOLocationID\") == F.col(\"dz.LocationID\"),\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        \"t.*\",\n",
    "        F.col(\"dz.zone\").alias(\"dropoff_zone\"),\n",
    "        F.col(\"dz.borough\").alias(\"dropoff_borough\")\n",
    "    )\n",
    "    \n",
    "    # Calculer des m√©triques suppl√©mentaires\n",
    "    trips_with_zones = trips_with_zones.withColumn(\n",
    "        \"is_same_borough\",\n",
    "        F.when(\n",
    "            (F.col(\"pickup_borough\").isNotNull()) & \n",
    "            (F.col(\"dropoff_borough\").isNotNull()) &\n",
    "            (F.col(\"pickup_borough\") == F.col(\"dropoff_borough\")),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"is_airport_trip\",\n",
    "        F.when(\n",
    "            (F.col(\"pickup_zone\").contains(\"Airport\")) |\n",
    "            (F.col(\"dropoff_zone\").contains(\"Airport\")) |\n",
    "            (F.col(\"PULocationID\").isin([1, 132, 138])) |\n",
    "            (F.col(\"DOLocationID\").isin([1, 132, 138])),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    trips_with_zones.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"year\", \"month\") \\\n",
    "        .saveAsTable(f\"{catalog}.silver.trips_with_zones\")\n",
    "    \n",
    "    print(f\"‚úÖ {catalog}.silver.trips_with_zones cr√©√©e: {trips_with_zones.count():,} lignes\")\n",
    "    \n",
    "    # 3. Enrichissement avec m√©t√©o\n",
    "    print(\"\\nüå§Ô∏è  √âtape 3: Enrichissement avec m√©t√©o...\")\n",
    "    weather_df = spark.table(weather_table)\n",
    "    \n",
    "    # Ajouter l'heure √† la m√©t√©o\n",
    "    weather_with_hour = weather_df.withColumn(\n",
    "        \"weather_hour\", F.hour(\"time\")\n",
    "    ).withColumn(\n",
    "        \"weather_date\", F.to_date(\"time\")\n",
    "    )\n",
    "    \n",
    "    # Jointure m√©t√©o - CORRECTION ICI\n",
    "    # On utilise un alias pour le DataFrame de gauche et on s√©lectionne explicitement les colonnes\n",
    "    trips_for_weather = trips_with_zones.withColumn(\n",
    "        \"pickup_hour\", F.hour(\"tpep_pickup_datetime\")\n",
    "    )\n",
    "    \n",
    "    # Faire la jointure avec alias\n",
    "    trips_complete = trips_for_weather.alias(\"t\").join(\n",
    "        weather_with_hour.alias(\"w\"),\n",
    "        (F.col(\"t.pickup_date\") == F.col(\"w.weather_date\")) &\n",
    "        (F.col(\"t.pickup_hour\") == F.col(\"w.weather_hour\")),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Maintenant, s√©lectionner les colonnes explicitement\n",
    "    # D'abord, obtenir toutes les colonnes du DataFrame de gauche\n",
    "    left_columns = [F.col(f\"t.{col}\") for col in trips_for_weather.columns]\n",
    "    \n",
    "    # S√©lectionner toutes les colonnes de gauche + les colonnes de m√©t√©o qu'on veut\n",
    "    trips_complete = trips_complete.select(\n",
    "        *left_columns,\n",
    "        F.col(\"w.temp\").alias(\"temperature\"),\n",
    "        F.col(\"w.rhum\").alias(\"humidity\"),\n",
    "        F.col(\"w.prcp\").alias(\"precipitation\"),\n",
    "        F.col(\"w.wspd\").alias(\"wind_speed\"),\n",
    "        F.col(\"w.pres\").alias(\"pressure\")\n",
    "    )\n",
    "    \n",
    "    # Nettoyer les colonnes temporaires\n",
    "    trips_complete = trips_complete.drop(\"pickup_hour\")\n",
    "    \n",
    "    # Ajouter des features m√©t√©o d√©riv√©es\n",
    "    trips_complete = trips_complete.withColumn(\n",
    "        \"is_rainy\",\n",
    "        F.when(F.col(\"precipitation\") > 0.1, 1).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"is_cold\",\n",
    "        F.when(F.col(\"temperature\") < 32, 1).otherwise(0)  # < 0¬∞C\n",
    "    ).withColumn(\n",
    "        \"is_hot\",\n",
    "        F.when(F.col(\"temperature\") > 86, 1).otherwise(0)   # > 30¬∞C\n",
    "    )\n",
    "    \n",
    "    # Statistiques de jointure\n",
    "    total_trips = trips_complete.count()\n",
    "    trips_with_weather = trips_complete.filter(F.col(\"temperature\").isNotNull()).count()\n",
    "    \n",
    "    print(f\"   ‚Üí Trips avec donn√©es m√©t√©o: {trips_with_weather:,}/{total_trips:,} ({(trips_with_weather/total_trips*100):.2f}%)\")\n",
    "    \n",
    "    trips_complete.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"year\", \"month\") \\\n",
    "        .saveAsTable(f\"{catalog}.silver.trips_complete\")\n",
    "    \n",
    "    print(f\"‚úÖ {catalog}.silver.trips_complete cr√©√©e: {trips_complete.count():,} lignes\")\n",
    "    \n",
    "    # 4. Cr√©er des tables de r√©sum√©\n",
    "    print(\"\\nüìà √âtape 4: Cr√©ation des tables de r√©sum√©...\")\n",
    "    \n",
    "    # R√©sum√© quotidien\n",
    "    daily_summary = trips_complete.groupBy(\n",
    "        \"pickup_date\",\n",
    "        \"pickup_borough\",\n",
    "        \"is_weekend\"\n",
    "    ).agg(\n",
    "        F.count(\"*\").alias(\"trip_count\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "        F.avg(\"trip_duration_minutes\").alias(\"avg_duration\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_fare\"),\n",
    "        F.sum(\"passenger_count\").alias(\"total_passengers\"),\n",
    "        F.avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "        F.avg(\"precipitation\").alias(\"avg_precipitation\")\n",
    "    ).withColumn(\n",
    "        \"revenue_per_trip\",\n",
    "        F.col(\"total_revenue\") / F.col(\"trip_count\")\n",
    "    )\n",
    "    \n",
    "    daily_summary.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"pickup_date\") \\\n",
    "        .saveAsTable(f\"{catalog}.silver.daily_summary\")\n",
    "    \n",
    "    print(f\"‚úÖ {catalog}.silver.daily_summary cr√©√©e\")\n",
    "    \n",
    "    # Afficher un r√©sum√©\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä R√âSUM√â SILVER LAYER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    tables = [\n",
    "        (f\"{catalog}.silver.trips_enriched\", trips_cleaned.count()),\n",
    "        (f\"{catalog}.silver.trips_with_zones\", trips_with_zones.count()),\n",
    "        (f\"{catalog}.silver.trips_complete\", trips_complete.count()),\n",
    "        (f\"{catalog}.silver.daily_summary\", daily_summary.count())\n",
    "    ]\n",
    "    \n",
    "    for table_name, count in tables:\n",
    "        print(f\"   üìÅ {table_name}: {count:,} lignes\")\n",
    "    \n",
    "    # Aper√ßu des donn√©es\n",
    "    print(\"\\nüëÅÔ∏è  Aper√ßu des donn√©es enrichies:\")\n",
    "    trips_complete.select(\n",
    "        \"pickup_date\",\n",
    "        \"pickup_zone\",\n",
    "        \"dropoff_zone\",\n",
    "        \"trip_distance\",\n",
    "        \"trip_duration_minutes\",\n",
    "        \"total_amount\",\n",
    "        \"temperature\",\n",
    "        \"is_rainy\"\n",
    "    ).limit(5).show()\n",
    "    \n",
    "    print(\"\\nüéâ PIPELINE SILVER TERMIN√â AVEC SUCC√àS!\")\n",
    "    \n",
    "    return trips_complete\n",
    "\n",
    "# Ex√©cuter\n",
    "create_silver_tables_unified()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f49244-a337-4c53-9fe1-50cc5a5e826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç V√âRIFICATION RAPIDE SILVER\n",
      "\n",
      "üîç V√©rification du catalogue: local\n",
      "   ‚úÖ Tables trouv√©es dans local.silver:\n",
      "      ‚Ä¢ daily_summary: 2,487 lignes\n",
      "      ‚Ä¢ trips_complete: 35,607,955 lignes\n",
      "      ‚Üí Colonnes: 42\n",
      "      ‚Üí Exemple: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance']...\n",
      "      ‚Ä¢ trips_enriched: 35,595,953 lignes\n",
      "      ‚Ä¢ trips_with_zones: 35,607,955 lignes\n",
      "\n",
      "‚úÖ Catalogue Silver actif: local\n",
      "\n",
      "üëÅÔ∏è  Aper√ßu de local.silver.trips_complete:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+----+-----+---+---------------------+------------------+----------+-----------+-----------+-----------+-------------------+--------------+--------------------+---------------+---------------+---------------+-----------+--------+-------------+----------+--------+--------+-------+------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|year|month|day|trip_duration_minutes|         speed_mph|is_weekend|hour_of_day|pickup_date|pickup_time|        pickup_zone|pickup_borough|        dropoff_zone|dropoff_borough|is_same_borough|is_airport_trip|temperature|humidity|precipitation|wind_speed|pressure|is_rainy|is_cold|is_hot|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+----+-----+---+---------------------+------------------+----------+-----------+-----------+-----------+-------------------+--------------+--------------------+---------------+---------------+---------------+-----------+--------+-------------+----------+--------+--------+-------+------+\n",
      "|       2| 2024-10-01 00:30:44|  2024-10-01 00:48:26|              1|          3.0|         1|                 N|         162|         246|           1|       18.4|  1.0|    0.5|       1.5|         0.0|                  1.0|        24.9|                 2.5|        0.0|2024|   10|  1|                 17.7| 10.16949152542373|         0|          0| 2024-10-01|   00:30:44|       Midtown East|     Manhattan|West Chelsea/Huds...|      Manhattan|              1|              0|       14.0|      81|          0.0|       6.0|  1018.0|       0|      1|     0|\n",
      "|       1| 2024-10-01 00:12:20|  2024-10-01 00:25:25|              1|          2.2|         1|                 N|          48|         236|           1|       14.2|  3.5|    0.5|       3.8|         0.0|                  1.0|        23.0|                 2.5|        0.0|2024|   10|  1|   13.083333333333334|10.089171974522294|         0|          0| 2024-10-01|   00:12:20|       Clinton East|     Manhattan|Upper East Side N...|      Manhattan|              1|              0|       14.0|      81|          0.0|       6.0|  1018.0|       0|      1|     0|\n",
      "|       1| 2024-10-01 00:04:46|  2024-10-01 00:13:52|              1|          2.7|         1|                 N|         142|          24|           1|       13.5|  3.5|    0.5|       3.7|         0.0|                  1.0|        22.2|                 2.5|        0.0|2024|   10|  1|                  9.1|17.802197802197803|         0|          0| 2024-10-01|   00:04:46|Lincoln Square East|     Manhattan|        Bloomingdale|      Manhattan|              1|              0|       14.0|      81|          0.0|       6.0|  1018.0|       0|      1|     0|\n",
      "|       1| 2024-10-01 00:12:10|  2024-10-01 00:23:01|              1|          3.1|         1|                 N|         233|          75|           1|       14.2|  3.5|    0.5|       2.0|         0.0|                  1.0|        21.2|                 2.5|        0.0|2024|   10|  1|                10.85|17.142857142857146|         0|          0| 2024-10-01|   00:12:10|UN/Turtle Bay South|     Manhattan|   East Harlem South|      Manhattan|              1|              0|       14.0|      81|          0.0|       6.0|  1018.0|       0|      1|     0|\n",
      "|       2| 2024-10-01 00:31:20|  2024-10-01 00:36:00|              2|         0.97|         1|                 N|         137|         137|           1|        7.2|  1.0|    0.5|      2.44|         0.0|                  1.0|       14.64|                 2.5|        0.0|2024|   10|  1|    4.666666666666667|12.471428571428572|         0|          0| 2024-10-01|   00:31:20|           Kips Bay|     Manhattan|            Kips Bay|      Manhattan|              1|              0|       14.0|      81|          0.0|       6.0|  1018.0|       0|      1|     0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+----+-----+---+---------------------+------------------+----------+-----------+-----------+-----------+-------------------+--------------+--------------------+---------------+---------------+---------------+-----------+--------+-------------+----------+--------+--------+-------+------+\n",
      "\n",
      "\n",
      "üîç V√©rification des donn√©es manquantes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ pickup_zone: 114,033 NULL (0.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ dropoff_zone: 302,365 NULL (0.8%)\n",
      "   ‚Ä¢ temperature: 820,984 NULL (2.3%)\n"
     ]
    }
   ],
   "source": [
    "def quick_check_silver():\n",
    "    \"\"\"V√©rification rapide des tables Silver\"\"\"\n",
    "    \n",
    "    print(\"üîç V√âRIFICATION RAPIDE SILVER\")\n",
    "    \n",
    "    # Essayer les deux catalogues\n",
    "    for catalogue in [\"local\", \"iceberg\"]:\n",
    "        print(f\"\\nüîç V√©rification du catalogue: {catalogue}\")\n",
    "        \n",
    "        try:\n",
    "            tables = spark.sql(f\"SHOW TABLES IN {catalogue}.silver\").collect()\n",
    "            \n",
    "            if not tables:\n",
    "                print(f\"   ‚ùå Aucune table dans {catalogue}.silver\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"   ‚úÖ Tables trouv√©es dans {catalogue}.silver:\")\n",
    "            \n",
    "            for table in tables:\n",
    "                table_name = table['tableName']\n",
    "                full_name = f\"{catalogue}.silver.{table_name}\"\n",
    "                \n",
    "                try:\n",
    "                    count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {full_name}\").collect()[0]['cnt']\n",
    "                    print(f\"      ‚Ä¢ {table_name}: {count:,} lignes\")\n",
    "                    \n",
    "                    # Afficher le sch√©ma pour trips_complete\n",
    "                    if table_name == \"trips_complete\":\n",
    "                        df = spark.table(full_name)\n",
    "                        print(f\"      ‚Üí Colonnes: {len(df.columns)}\")\n",
    "                        print(f\"      ‚Üí Exemple: {df.columns[:5]}...\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"      ‚Ä¢ {table_name}: Erreur - {e}\")\n",
    "            \n",
    "            return catalogue  # Retourne le premier catalogue avec des tables\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erreur avec {catalogue}: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Ex√©cution\n",
    "catalogue = quick_check_silver()\n",
    "\n",
    "if catalogue:\n",
    "    print(f\"\\n‚úÖ Catalogue Silver actif: {catalogue}\")\n",
    "    \n",
    "    # Afficher un √©chantillon\n",
    "    print(f\"\\nüëÅÔ∏è  Aper√ßu de {catalogue}.silver.trips_complete:\")\n",
    "    spark.table(f\"{catalogue}.silver.trips_complete\").limit(5).show()\n",
    "    \n",
    "    # V√©rifier les donn√©es manquantes\n",
    "    trips_df = spark.table(f\"{catalogue}.silver.trips_complete\")\n",
    "    print(f\"\\nüîç V√©rification des donn√©es manquantes:\")\n",
    "    \n",
    "    for col in [\"pickup_zone\", \"dropoff_zone\", \"temperature\"]:\n",
    "        null_count = trips_df.filter(F.col(col).isNull()).count()\n",
    "        total = trips_df.count()\n",
    "        percentage = (null_count / total) * 100 if total > 0 else 0\n",
    "        print(f\"   ‚Ä¢ {col}: {null_count:,} NULL ({percentage:.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå Aucune table Silver trouv√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "337e98c2-6817-4660-8279-9f0f6e4c7771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√âMARRAGE DU PIPELINE GOLD - ANALYTICS & ML\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üöÄ D√âMARRAGE DU PIPELINE GOLD - ANALYTICS & ML\n",
      "======================================================================\n",
      "üîç D√©tection du catalogue Silver...\n",
      "   ‚úÖ Catalogue trouv√©: local\n",
      "\n",
      "üìÅ Catalogue Silver d√©tect√©: local\n",
      "‚úÖ Namespace local.gold cr√©√©/valid√©\n",
      "\n",
      "üìä Cr√©ation des tables analytiques Gold...\n",
      "\n",
      "   üìÖ Cr√©ation de daily_metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Table gold.daily_metrics cr√©√©e: 374 lignes\n",
      "\n",
      "   üïê Cr√©ation de hourly_patterns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Table gold.hourly_patterns cr√©√©e: 8,796 lignes\n",
      "\n",
      "   üó∫Ô∏è  Cr√©ation de top_routes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Table gold.top_routes cr√©√©e: 1,000 lignes\n",
      "\n",
      "   üöï Cr√©ation de driver_performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Table gold.driver_performance cr√©√©e: 740 lignes\n",
      "\n",
      "   üå§Ô∏è  Cr√©ation de weather_impact...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Table gold.weather_impact cr√©√©e: 527 lignes\n",
      "\n",
      "   üí≥ Cr√©ation de payment_analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Table gold.payment_analysis cr√©√©e: 1,476 lignes\n",
      "\n",
      "ü§ñ √âtape 2: Cr√©ation du dataset ML features...\n",
      "   Donn√©es totales: 35,607,955 lignes\n",
      "   √âchantillon ML: 3,560,139 lignes (10% des donn√©es)\n",
      "\n",
      "   üîß Feature engineering avanc√©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Dataset ML cr√©√©: 3,560,139 lignes\n",
      "   üìä Nombre de features: 45\n",
      "\n",
      "   üìã Aper√ßu des features cr√©√©es:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+-----------+---------------+-------------+------------+------------+------------+-----------+----------+------------+---------------------+-------------------+----------+--------+-------+------+-----------+-------------+-----------+-----------+------------+-----------+-------------------+--------------------+-------------------+-------------------+-------------------+------------------+--------------------+-----------------------+--------------------+-------------------+-----------------------+----------+----------+--------+--------+---------------+---------------+--------+------------------+------------------+-------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|pickup_date|passenger_count|trip_distance|PULocationID|DOLocationID|payment_type|fare_amount|tip_amount|total_amount|trip_duration_minutes|          speed_mph|is_weekend|is_rainy|is_cold|is_hot|temperature|precipitation|hour_of_day|day_of_week|pickup_month|pickup_year|           hour_sin|            hour_cos|            day_sin|            day_cos|          month_sin|         month_cos|recent_trips_in_area|avg_recent_fare_in_area|   log_trip_distance|  log_trip_duration|distance_duration_ratio|rainy_cold|rainy_warm|dry_cold|dry_warm|is_morning_rush|is_evening_rush|is_night|     fare_per_mile|    tip_percentage|has_tip|\n",
      "+--------+--------------------+---------------------+-----------+---------------+-------------+------------+------------+------------+-----------+----------+------------+---------------------+-------------------+----------+--------+-------+------+-----------+-------------+-----------+-----------+------------+-----------+-------------------+--------------------+-------------------+-------------------+-------------------+------------------+--------------------+-----------------------+--------------------+-------------------+-----------------------+----------+----------+--------+--------+---------------+---------------+--------+------------------+------------------+-------+\n",
      "|       2| 2024-01-01 17:45:45|  2024-01-01 17:50:13| 2024-01-01|              1|         0.01|           1|           1|           1|      110.0|     27.75|      138.75|    4.466666666666667|0.13432835820895522|         0|       0|      1|     0|        7.0|          0.0|         17|          2|           1|       2024|-0.9659258262890683|-0.25881904510252063| 0.7818314824680298| 0.6234898018587336|0.49999999999999994|0.8660254037844387|                   0|                   NULL|0.009950330853168083|  1.698669046162043|   0.002238805970149254|         0|         0|       1|       0|              0|              1|       0|           11000.0|              20.0|      1|\n",
      "|       2| 2024-01-04 16:52:03|  2024-01-04 16:52:11| 2024-01-04|              3|         0.04|           1|           1|           1|      135.0|     15.55|      151.55|  0.13333333333333333|               18.0|         0|       0|      1|     0|        4.0|          0.0|         16|          5|           1|       2024|-0.8660254037844384| -0.5000000000000004| -0.433883739117558|-0.9009688679024191|0.49999999999999994|0.8660254037844387|                   0|                   NULL|0.039220713153281295|0.12516314295400602|                    0.3|         0|         0|       1|       0|              0|              0|       0|            3375.0|10.260640052787858|      1|\n",
      "|       2| 2024-01-07 14:25:05|  2024-01-07 14:25:11| 2024-01-07|              1|         0.02|           1|           1|           1|       20.0|       6.3|        27.3|                  0.1|               12.0|         1|       1|      1|     0|        3.0|          0.8|         14|          1|           1|       2024|-0.4999999999999997| -0.8660254037844388|                0.0|                1.0|0.49999999999999994|0.8660254037844387|                   0|                   NULL|0.019802627296179712|0.09531017980432487|    0.19999999999999998|         1|         0|       0|       0|              0|              0|       0|            1000.0|23.076923076923077|      1|\n",
      "|       2| 2024-01-07 14:56:31|  2024-01-07 15:08:40| 2024-01-07|              1|         4.87|           1|         265|           1|       35.9|      7.38|       44.28|                12.15| 24.049382716049383|         1|       1|      1|     0|        3.0|          0.8|         14|          1|           1|       2024|-0.4999999999999997| -0.8660254037844388|                0.0|                1.0|0.49999999999999994|0.8660254037844387|                   1|                   27.3|   1.769854633840005| 2.5764217586237734|     0.4008230452674897|         1|         0|       0|       0|              0|              0|       0| 7.371663244353182|16.666666666666664|      1|\n",
      "|       2| 2024-01-12 11:06:23|  2024-01-12 11:06:44| 2024-01-12|              1|         0.19|           1|         265|           1|       25.0|       6.5|        32.5|                 0.35|  32.57142857142858|         0|       0|      1|     0|       -1.0|          0.0|         11|          6|           1|       2024|  0.258819045102521| -0.9659258262890682|-0.9749279121818236|-0.2225209339563146|0.49999999999999994|0.8660254037844387|                   0|                   NULL|   0.173953307123438|0.30010459245033805|     0.5428571428571429|         0|         0|       1|       0|              0|              0|       0|131.57894736842104|              20.0|      1|\n",
      "+--------+--------------------+---------------------+-----------+---------------+-------------+------------+------------+------------+-----------+----------+------------+---------------------+-------------------+----------+--------+-------+------+-----------+-------------+-----------+-----------+------------+-----------+-------------------+--------------------+-------------------+-------------------+-------------------+------------------+--------------------+-----------------------+--------------------+-------------------+-----------------------+----------+----------+--------+--------+---------------+---------------+--------+------------------+------------------+-------+\n",
      "\n",
      "\n",
      "üéØ √âtape 3: Pr√©paration des datasets d'entra√Ænement ML...\n",
      "\n",
      "   1. Dataset: Pr√©diction du prix (regression)...\n",
      "\n",
      "   2. Dataset: Pr√©diction de la dur√©e (regression)...\n",
      "\n",
      "   3. Dataset: Pr√©diction du pourboire (classification)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Dataset prix cr√©√©: 3,560,139 lignes\n",
      "   ‚úÖ Dataset dur√©e cr√©√©: 3,560,139 lignes\n",
      "   ‚úÖ Dataset pourboire cr√©√©: 3,506,557 lignes\n",
      "\n",
      "üìä √âtape 4: Cr√©ation des tables pour dashboards...\n",
      "\n",
      "   1. Table: executive_dashboard...\n",
      "      ‚úÖ Table executive_dashboard cr√©√©e\n",
      "\n",
      "   2. Table: hourly_analysis...\n",
      "      ‚úÖ Table hourly_analysis cr√©√©e\n",
      "\n",
      "   3. Table: driver_analytics...\n",
      "      ‚úÖ Table driver_analytics cr√©√©e\n",
      "\n",
      "   4. Table: weather_dashboard...\n",
      "      ‚úÖ Table weather_dashboard cr√©√©e\n",
      "\n",
      "   5. Table: route_analysis...\n",
      "      ‚úÖ Table route_analysis cr√©√©e\n",
      "\n",
      "   6. Table: payment_dashboard...\n",
      "      ‚úÖ Table payment_dashboard cr√©√©e\n",
      "\n",
      "   üéØ Toutes les tables dashboard ont √©t√© cr√©√©es!\n",
      "      ‚Ä¢ executive_dashboard\n",
      "      ‚Ä¢ hourly_analysis\n",
      "      ‚Ä¢ driver_analytics\n",
      "      ‚Ä¢ weather_dashboard\n",
      "      ‚Ä¢ route_analysis\n",
      "      ‚Ä¢ payment_dashboard\n",
      "\n",
      "üîç √âtape 5: V√©rification de la qualit√© des donn√©es Gold...\n",
      "   ‚úÖ Table daily_business_metrics existe\n",
      "   ‚úÖ Table hourly_patterns existe\n",
      "   ‚úÖ Table top_routes existe\n",
      "   ‚úÖ Table driver_performance existe\n",
      "   ‚úÖ Table ml_tip_prediction existe\n",
      "   ‚úÖ M√©triques quotidiennes coh√©rentes\n",
      "\n",
      "üìä R√©sum√© qualit√© Gold: 6/6 checks pass√©s\n",
      "\n",
      "======================================================================\n",
      "üìä RAPPORT FINAL - GOLD LAYER\n",
      "======================================================================\n",
      "\n",
      "‚úÖ PIPELINE GOLD TERMIN√â AVEC SUCC√àS!\n",
      "‚è±Ô∏è  Dur√©e totale: 2 minutes et 21 secondes\n",
      "üìÅ Catalogue utilis√©: local\n",
      "\n",
      "üìà TABLES CR√â√âES:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ daily_metrics: 374 lignes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ hourly_patterns: 8,796 lignes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ top_routes: 1,000 lignes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ driver_performance: 740 lignes\n",
      "   ‚Ä¢ ml_dataset: 3,560,139 lignes\n",
      "   ‚Ä¢ ml_fare_prediction: 3,560,139 lignes\n",
      "   ‚Ä¢ ml_duration_prediction: 3,560,139 lignes\n",
      "   ‚Ä¢ ml_tip_prediction: 3,506,557 lignes\n",
      "\n",
      "üéØ QUALIT√â DES DONN√âES:\n",
      "   ‚úÖ Tables Gold v√©rifi√©es: 6/6\n",
      "\n",
      "üìä TABLES DASHBOARD CR√â√âES:\n",
      "   ‚Ä¢ executive_dashboard\n",
      "   ‚Ä¢ hourly_analysis\n",
      "   ‚Ä¢ driver_analytics\n",
      "   ‚Ä¢ weather_dashboard\n",
      "   ‚Ä¢ route_analysis\n",
      "   ‚Ä¢ payment_dashboard\n",
      "\n",
      "‚ú® PR√äT POUR L'ANALYSE AVANC√âE!\n",
      "======================================================================\n",
      "Vous pouvez maintenant:\n",
      "1. Explorer les tables Gold avec SQL\n",
      "2. Cr√©er des dashboards avec les tables dashboard\n",
      "3. Entra√Æner des mod√®les ML avec les datasets pr√©par√©s\n",
      "4. Analyser les performances des conducteurs\n",
      "5. √âtudier l'impact de la m√©t√©o sur les trajets\n",
      "======================================================================\n",
      "\n",
      "‚ú® Pr√™t pour l'analyse avanc√©e! Vous pouvez maintenant:\n",
      "   1. Explorer les tables Gold avec SQL\n",
      "   2. Connecter Superset aux vues cr√©√©es\n",
      "   3. Lancer des mod√®les ML sur les datasets pr√©par√©s\n",
      "   4. Analyser les m√©triques business dans les dashboards\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de14280-487c-4124-91cb-d79cd7399a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
